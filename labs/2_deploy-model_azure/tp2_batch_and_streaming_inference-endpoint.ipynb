{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# TP 2 : job d'entraînement, déploiement de modèle, prédiction par batch et en ligne, A/B Testing"
   ],
   "metadata": {
    "id": "title"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=https://www.headmind.com/wp-content/uploads/2024/01/logo_dark.png width=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=https://www.ensta-paris.fr/profiles/createur_profil/themes/createur/dist/images/logo_ensta_new.jpg.pagespeed.ce.ERsGv8BS3M.jpg width=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Objectifs\r\n",
    "\r\n",
    "Au cours de ce TP, vous apprendrez à créer un modèle de Vision personnalisé, vous configurerez et lancerez un job d'entraînement, enfin, vous déploierez le modèle pour de la prédiction par lot (batch) puis pour de la prédiction en temps réel (streaming). \r\n",
    "\r\n",
    "On utilisera les services d'Azure Machine Leaning suivant : \r\n",
    "- Azure ML Données\r\n",
    "- Azure ML Training\r\n",
    "- Azure ML Models\r\n",
    "- Azure ML Endpoints\r\n",
    "\r\n",
    "## Liens utiles\r\n",
    "\r\n",
    "- [Custom training](https://learn.microsoft.com/fr-fr/azure/machine-learning/how-to-train-model?view=azureml-api-2&source=recommendations&tabs=python) \r\n",
    "- [Endpoint en temps réel](https://learn.microsoft.com/fr-fr/azure/machine-learning/how-to-deploy-online-endpoints?view=azureml-api-2&source=recommendations&tabs=python)"
   ],
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset\r\n",
    "\r\n",
    "TensorFlow met à disposition des datasets issues de directement de leur librarie. (tensorflow.keras.datasets)\r\n",
    "\r\n",
    "Pour mettre tout ceci en pratique, on utilisera les données MNIST et on développera un modèle de Computer Vision avec TensorFlow afin de prédire le chiffre présent sur les images parmi 10 possibles : 0, 1, 2, 3, 4, 5, 6, 7, 8, 9."
   ],
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mise en place de l'environnement"
   ],
   "metadata": {
    "id": "3b1ffd5ab768"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installer les librairies nécessaires pour intéragir avec Azure ML et autres\r\n"
   ],
   "metadata": {
    "id": "aae9ca040eab"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {
    "id": "23e23ce735c9"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables de connexion à Azure ML"
   ],
   "metadata": {
    "id": "107c51893a64"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "SUBSCRIPTION_ID = \"_____\" \r\n",
    "RESOURCE_GROUP = \"_____\"\r\n",
    "WORKSPACE_NAME = \"_____\""
   ],
   "outputs": [],
   "metadata": {
    "id": "294fe4e5a671"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Connexion au workspace Azure ML pour utiliser le SDK Python Azure\r\n",
    "\r\n",
    "Le SDK python permet d'utiliser l'API d'Azure avec du code plutôt qu'avec l'interface utilisateur."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml import MLClient\r\n",
    "from azure.identity import DefaultAzureCredential\r\n",
    "\r\n",
    "ml_client = MLClient(DefaultAzureCredential(), SUBSCRIPTION_ID, RESOURCE_GROUP, WORKSPACE_NAME)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import des librairies python"
   ],
   "metadata": {
    "id": "setup_vars"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 1: Entraînement d'un modèle\r\n",
    "\r\n",
    "Nous allons maintenant créer un modèle de DL et nous l'entraînerons sur les données MNIST. Ce sont des images de digit en niveau de gris."
   ],
   "metadata": {
    "id": "tutorial_start:custom"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Script d'entrainement\r\n",
    "\r\n",
    "Le script d'entraînement est dans src/tf_mnist.py"
   ],
   "metadata": {
    "id": "taskpy_contents"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Créer une ressource de calcul\r\n",
    "\r\n",
    "Nous allons maintenant définir un job d'entraînement une ressource de calcul. Azure ML a besoin de cette dernière pour lancer des entraînements.\r\n",
    "\r\n",
    "On provisionne ici un cluster de calcul basé sur Linux. Il y a de nombreuses séries de cluster, utilisant des CPUs, d'autres des GPUs, une certaine quantité de mémoire, de RAM...\r\n",
    "Toutes ces caractéristiques ont aussi un rapport avec le prix horaire quand on loue le cluster. [Pricing cluster de calcul](https://azure.microsoft.com/fr-fr/pricing/details/machine-learning/)\r\n",
    "\r\n",
    "Nous allons entraîner un modèle de Deep Learning, l'idéal est d'avoir du GPU, pour un entraînement efficace et rapide. Si, c'est possible (cela dépend de quota de coeurs GPU attribué à l'abonnement Azure), nous allons créer un cluster de taille NC8asT4v3. Si on a pas de coeur GPU, nous allons choisir le cluster suivant : STANDARD_F72s_v2\r\n",
    "\r\n",
    "On utilise l'objet `AmlCompute` qui prend les paramètres suivant :\r\n",
    "- name : Nom du cluster dans Azure ML (doit être unique dans le workspace).\r\n",
    "- type : Type de ressource (toujours \"amlcompute\" pour ce type de cluster).\r\n",
    "- size : Taille de la VM utilisé\r\n",
    "- min_instances : Nombre minimal de nœuds actifs (0 signifie que le cluster peut s'arrêter complètement quand il est inactif pour économiser des coûts).\r\n",
    "- max_instances : Nombre maximal de nœuds que le cluster peut scaler (ici, 4 VM maximum en parallèle)\r\n",
    "- idle_time_before_scale_down : Délai en secondes avant de rendre le cluster inactif.\r\n",
    "- tier : Priorité de la VM, 2 choix : \"Dedicated\" pour une VM dédiée (plus chères, garanties jusqu'à la fin du job) et \"LowPriority\" : Moins cher (~5x moins cher), mais risque d'interruption"
   ],
   "metadata": {
    "id": "train_custom_job"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import AmlCompute\r\n",
    "\r\n",
    "gpu_cluster = AmlCompute(\r\n",
    "    name=\"_____\",\r\n",
    "    type=\"amlcompute\",\r\n",
    "    size=\"STANDARD_F72s_v2\", \r\n",
    "    min_instances=0,\r\n",
    "    max_instances=1,\r\n",
    "    idle_time_before_scale_down=180,\r\n",
    "    tier=\"LowPriority\",\r\n",
    ")\r\n",
    "\r\n",
    "# Créer le cluster\r\n",
    "gpu_cluster = ml_client.begin_create_or_update(gpu_cluster).result()"
   ],
   "outputs": [],
   "metadata": {
    "id": "mxIxvDdglugx"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veuillez vérifier que le cluster est bien crée en ouvrant le portail Azure ML sur votre navigateur. Il doit avoir 0 noeud de calcul."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Configurer et soumettre l'entraînement du modèle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Obtenir les données d'entraînement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.datasets import mnist\r\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "\r\n",
    "mnist_df = pd.DataFrame(X_train.reshape(X_train.shape[0], -1))\r\n",
    "mnist_df['label'] = y_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Représentation du dataframe :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mnist_df_test = pd.DataFrame(X_test.reshape(X_test.shape[0], -1))\r\n",
    "mnist_df_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "y_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Visualisation des données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "# 2. Sélectionner 10 images aléatoires\r\n",
    "indices = np.random.choice(len(X_train), 10, replace=False)\r\n",
    "sample_images = X_train[indices]\r\n",
    "sample_labels = y_train[indices]\r\n",
    "\r\n",
    "# 3. Visualisation des images (optionnel)\r\n",
    "plt.figure(figsize=(10, 3))\r\n",
    "for i in range(10):\r\n",
    "    plt.subplot(2, 5, i+1)\r\n",
    "    plt.imshow(sample_images[i], cmap='gray')\r\n",
    "    plt.title(f\"Label: {sample_labels[i]}\")\r\n",
    "    plt.axis('off')\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Processing des données pour l'entraînement\r\n",
    "\r\n",
    "Ce script Python permet de convertir des images et leurs étiquettes dans le format IDX, un format binaire couramment utilisé pour les datasets d'entraînement comme MNIST"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import struct\r\n",
    "import gzip\r\n",
    "import os\r\n",
    "\r\n",
    "# Fonction pour sauvegarder les images au format IDX\r\n",
    "def save_images(images, filename):\r\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\r\n",
    "    with gzip.open(filename, 'wb') as f:\r\n",
    "        # Écrire l'en-tête\r\n",
    "        magic_number = 2051  # pour les images\r\n",
    "        num_images = len(images)\r\n",
    "        num_rows = images.shape[1]\r\n",
    "        num_cols = images.shape[2]\r\n",
    "        \r\n",
    "        f.write(struct.pack('>IIII', magic_number, num_images, num_rows, num_cols))\r\n",
    "        \r\n",
    "        # Écrire les données\r\n",
    "        f.write(images.astype(np.uint8).tobytes())\r\n",
    "\r\n",
    "# Fonction pour sauvegarder les étiquettes au format IDX\r\n",
    "def save_labels(labels, filename):\r\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\r\n",
    "    with gzip.open(filename, 'wb') as f:\r\n",
    "        # Écrire l'en-tête\r\n",
    "        magic_number = 2049  # pour les étiquettes\r\n",
    "        num_items = len(labels)\r\n",
    "        \r\n",
    "        f.write(struct.pack('>II', magic_number, num_items))\r\n",
    "        \r\n",
    "        # Écrire les données\r\n",
    "        f.write(labels.astype(np.uint8).tobytes())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sauvegarder les données d'entraînement et de test\r\n",
    "\r\n",
    "Le nom des fichiers est nécessiare ensuite pour le script d'entraînement. Exécuter simplement la cellule."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_images(X_train, \"./mnist_gzip/mnist/raw/train-images-idx3-ubyte.gz\")\r\n",
    "save_labels(y_train, \"./mnist_gzip/mnist/raw/train-labels-idx1-ubyte.gz\")\r\n",
    "save_images(X_test, \"./mnist_gzip/mnist/raw/t10k-images-idx3-ubyte.gz\")\r\n",
    "save_labels(y_test, \"./mnist_gzip/mnist/raw/t10k-labels-idx1-ubyte.gz\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Enregistrer le jeu de données Azure ML\r\n",
    "\r\n",
    "Nous allons utiliser la section \"Données\" d'Azure Machine Learning pour stocker, gérer et versionner les données utilisés pour l'entraînement, la validation et l'inférence de modèles ML\r\n",
    "\r\n",
    "On utilise l'objet `Data` qui prend les paramètres suivant :\r\n",
    "- name : Nom que l'on donne à notre répertoire de données\r\n",
    "- type : Type d'asset\r\n",
    "- path : Chemin dans notre local ou se trouve nos données\r\n",
    "- description : Phrase qui décrit nos données"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import Data\r\n",
    "from azure.ai.ml.constants import AssetTypes\r\n",
    "\r\n",
    "mnist_data = Data(\r\n",
    "    path=\"./mnist_gzip\",\r\n",
    "    type=AssetTypes.URI_FOLDER,\r\n",
    "    name=\"mnist_gzip_dataset\",\r\n",
    "    description=\"MNIST en gzip\"\r\n",
    ")\r\n",
    "\r\n",
    "ml_client.data.create_or_update(mnist_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Créer un environnement pour le job\r\n",
    "\r\n",
    "Un envrionnement est un conteneur qui encapsule toutes les dépendances (librairies Python, packages, variables d’environnement et configurations) nécessaires pour exécuter un script\r\n",
    "\r\n",
    "Certains sont fournis directement par Azure Machine Learning, on va en utiliser un qui inclut tensorflow : \"AzureML-tensorflow-2.12-cuda11@latest\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "curated_env_name = \"AzureML-tensorflow-2.12-cuda11@latest\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entraînement\r\n",
    "\r\n",
    "L'entraîenment se fait via un script (script d'entraînement) qui est ici : src/tf_mnist.py et a été réalisé au préalable.\r\n",
    "\r\n",
    "La fonctionne `command` est utilisé ici, elle prend en argument :\r\n",
    "- inputs : Dictionnaire qui définit toutes les entrées du job\r\n",
    "  - data_folder : Chemin vers les données MNIST\r\n",
    "  - batch_size : Taille des lots pour l'entraînement (on prendra 64)\r\n",
    "  - first_layer_neurons : Nombre de neurones dans la première couche (on prendra 256)\r\n",
    "  - second_layer_neurons : Nombre de neurones dans la seconde couche (on prendra 128)\r\n",
    "  - learning_rate : Taux d'apprentissage du modèle (on prendra 0.01)\r\n",
    "- compute : Nom du cluster GPU qui exécutera le job d'entraînement (créer précédemment)\r\n",
    "- environment : Environnement d'exécution préconfiguré (curated_env_name)\r\n",
    "- code : Chemin vers le répertoire contenant le code source (\"./src/\")\r\n",
    "- command : Commande à exécuter avec les paramètres d'entrée\r\n",
    "- experiment_name : Nom de l'expérience pour organiser les jobs dans Azure ML\r\n",
    "- display_name : Nom du job affiché dans l'interface Azure ML"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "paramètre à renseigner ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml import command\r\n",
    "from azure.ai.ml import Input\r\n",
    "\r\n",
    "job = command(\r\n",
    "    inputs=dict(\r\n",
    "        data_folder=Input(type=\"uri_folder\", path=mnist_data.path),\r\n",
    "        batch_size=__, # à remplir \r\n",
    "        first_layer_neurons=__, # à remplir \r\n",
    "        second_layer_neurons=__, # à remplir \r\n",
    "        learning_rate=__, # à remplir \r\n",
    "    ),\r\n",
    "    compute=gpu_cluster.name,\r\n",
    "    environment=curated_env_name,\r\n",
    "    code=\"./src/\",\r\n",
    "    command=\"python tf_mnist.py --data-folder ${{inputs.data_folder}} --batch-size ${{inputs.batch_size}} --first-layer-neurons ${{inputs.first_layer_neurons}} --second-layer-neurons ${{inputs.second_layer_neurons}} --learning-rate ${{inputs.learning_rate}}\",\r\n",
    "    experiment_name=\"_____\",\r\n",
    "    display_name=\"_____\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lancer l'entraînement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "returned_job = ml_client.jobs.create_or_update(job)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Que se passe-t-il pendant l'exécution d'un entraînement sur Azure ?\r\n",
    "Lors de son exécution, le travail passe par les étapes suivantes :\r\n",
    "\r\n",
    "Préparation : Une image docker est créée en fonction de l'environnement défini. L'image est téléchargée dans le registre des conteneurs de le workspace.e\r\n",
    "\r\n",
    "Mise à l'échelle : Le cluster \"s'active\" et devient disponible pour exécuter la tâche, il prend alors un noeud de calcul.\r\n",
    "\r\n",
    "Exécution : Tous les scripts du dossier de scripts src sont téléchargés, les données sont copiées, et le script est exécuté. Les sorties de stdout et du dossier ./logs sont transmises à l'historique du travail et peuvent être utilisées pour surveiller le travail.\r\n",
    "\r\n",
    "=> Temps d'entraînement : 9min (sans GPU)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tuning des hyperparamètres\r\n",
    "\r\n",
    "Maintenant, voyons si on peut améliorer la précision de notre modèle. On peut régler et optimiser les hyperparamètres du modèle à l'aide du balayage d'Azure ML.\r\n",
    "\r\n",
    "Pour ajuster les hyperparamètres du modèle, il faut définir l'espace de paramètres dans lequel on souhaite effectuer des recherches pendant l'entraînement. Pour ce faire, il faut remplacer certains des paramètres (batch_size, first_layer_neurons, second_layer_neurons et learning_rate) transmis à l'entraînement par des entrées spéciales du package azure.ml.sweep."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.sweep import Choice, LogUniform\r\n",
    "\r\n",
    "job_for_sweep = job(\r\n",
    "    batch_size=Choice(values=[32, 64, 128]), # Choix parmis 3 valeurs possibles\r\n",
    "    first_layer_neurons=Choice(values=[16, 64, 128, 256, 512]), # Choix parmis 5 valeurs possibles\r\n",
    "    second_layer_neurons=Choice(values=[16, 64, 256, 512]), # Choix parmis 4 valeurs possibles\r\n",
    "    learning_rate=LogUniform(min_value=-6, max_value=-1), # Distribution uniforme logarithmique 10^-6 et 10^-1\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On utilise la fonction `sweep` qui va prendre en argument:\r\n",
    "- compute : Cluster utilisé pour l'entraînement\r\n",
    "- sampling_algorithm :  Détermine la méthode d'échantillonnage des hyperparamètres\r\n",
    "- primary_metric : Indique la métrique principale à optimiser pendant le balayage (ici on veut optimiser la métrique \"validation_acc\" qui est la précision sur l'ensemble de validation)\r\n",
    "- goal : Spécifie si la primary_metric doit être maximisée ou minimisée\r\n",
    "- max_total_trials :  Nombre maximum total d'expériences à exécuter\r\n",
    "- max_concurrent_trials : Nombre maximum d'expériences pouvant s'exécuter en parallèle\r\n",
    "- early_termination_policy : Configure la politique d'arrêt anticipé des essais non prometteurs (ici on utilisera BanditPolicy)\r\n",
    "\r\n",
    "La politique Bandit utilise : \r\n",
    "- slack_factor : Seuil de tolérance par rapport au meilleur essai (on prendra 10% donc on arretera les essais si leurs performances sont inférieures de plus de 10% au meilleur essai)\r\n",
    "- evaluation_interval : Fréquence d'évaluation de la politique d'arrêt anticipé (on prendra 2)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.sweep import BanditPolicy\r\n",
    "\r\n",
    "sweep_job = job_for_sweep.sweep(\r\n",
    "    compute=gpu_cluster.name,\r\n",
    "    sampling_algorithm=\"random\",\r\n",
    "    primary_metric=\"validation_acc\",\r\n",
    "    goal=\"Maximize\",\r\n",
    "    max_total_trials=8,\r\n",
    "    max_concurrent_trials=4,\r\n",
    "    early_termination_policy=BanditPolicy(slack_factor=0.1, evaluation_interval=2),\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lancement des entraînements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temps d'exécution : ~ 20/25 min"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "returned_sweep_job = ml_client.create_or_update(sweep_job)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arrêter le cluster de calcul\r\n",
    "\r\n",
    "Comme le cluster est facturé au temps d'utilisation, pensez à l'éteindre dès qu'il n'est plus nécessaire pour limiter les dépenses."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.compute.begin_delete(gpu_cluster.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inscrire dans Azure le modèle entrainé\r\n",
    "\r\n",
    "En allant sur le portail Azure Machine Learning, vous trouverez facilement, l'entraînement qui a produit le meilleur modèle. Il faut récupérer le nom de cet entraînement."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Nom du job ayant produit le meilleur modèle\r\n",
    "run =  \"_____\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On va utiliser l'objet `Model` pour enregistrer notre modèle, il comprend comme argument :\r\n",
    "- path : Spécifie le chemin où le modèle entraîné est stocké dans Azure ML (ici il est dans une output d'entraînement)\r\n",
    "- name : Nom unique pour le modèle\r\n",
    "- description : Description du modèle\r\n",
    "- type : Spécifie le type de modèle à enregistrer (ici c'est un \"custom_model\")"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import Model\r\n",
    "\r\n",
    "# Enregistrement du modèle\r\n",
    "model = Model(\r\n",
    "    path=f\"azureml://jobs/{run}/outputs/artifacts/paths/outputs/model/\",\r\n",
    "    name=\"_____\", \r\n",
    "    description=\"_____\",\r\n",
    "    type=\"custom_model\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inscription du modèle dans Azure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "registered_model = ml_client.models.create_or_update(model=model)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# PART 2 : Déploiement en temps réel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Inférence en temps réel\r\n",
    "\r\n",
    "Nous allons maintenant utiliser le modèle pour faire de la prédiction en temps réel. Il faut déployer ce modèle sur un endpoint.\r\n",
    "\r\n",
    "L'endpoint est un serveur qui tourne sur les ressources qu'on définit au préalable. Il gère les requêtes qu'il reçoit au format HTTP et distribue les calculs pour une réponse avec un minimum de latence. On expose donc un modèle via une API, permettant aux applications d'envoyer des données et de recevoir des prédictions.\r\n",
    "\r\n",
    "Dans un premier temps, on va créer un endpoint de temps réel : `ManagedOnlineEndpoint`, qui comprend :\r\n",
    "- name : Nom de l'endpoint\r\n",
    "- description : Description de l'endpoint\r\n",
    "- auth_mode : Mode d'authentification pour sécuriser l'accès à l'endpoint (\"token\" ou \"key\")"
   ],
   "metadata": {
    "id": "make_prediction"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création d'un endpoint"
   ],
   "metadata": {
    "id": "get_test_item:test"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "online_endpoint_name = \"_____\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import (\r\n",
    "    ManagedOnlineEndpoint,\r\n",
    "    ManagedOnlineDeployment,\r\n",
    "    Model\r\n",
    ")\r\n",
    "\r\n",
    "# create an online endpoint\r\n",
    "endpoint = ManagedOnlineEndpoint(\r\n",
    "    name=online_endpoint_name,\r\n",
    "    description=\"_____\",\r\n",
    "    auth_mode=\"key\",\r\n",
    ")\r\n",
    "\r\n",
    "endpoint = ml_client.begin_create_or_update(endpoint).result()\r\n",
    "\r\n",
    "print(f\"Endpoint {endpoint.name} provisioning state: {endpoint.provisioning_state}\")"
   ],
   "outputs": [],
   "metadata": {
    "id": "E1EQBPGnlugz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "On peut vérifier la bonne création de l'endpoint via code ou via le portail Azure ML."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\r\n",
    "\r\n",
    "print(f'Endpoint \"{endpoint.name}\" with provisioning state \"{endpoint.provisioning_state}\" is retrieved')"
   ],
   "outputs": [],
   "metadata": {
    "id": "cl59KGnXlugz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Déployer le modèle sur l'endpoint\r\n",
    "\r\n",
    "Un déploiement sur un endpoint Azure ML est l'allocation d'une VM qui héberge votre modèle et le rend accessible via une API pour inférence.\r\n",
    "\r\n",
    "Pour le déploiement, on a besoin d'un script de scoring. Il est ici : src/score.py\r\n",
    "Le script de scoring comprend 2 fonctions : la fonction `init` qui lance le modèle, et la fonction `run` qui définit comment le modèle doit processer les données d'entrée quand on fait un appel à l'endpoint, et quelles doit être la sortie du modèle.\r\n",
    "\r\n",
    "Pour le déploiement on utilise l'objet `ManagedOnlineDeployment`, qui prend en argument :\r\n",
    "- name : Nom unique du déploiement dans l'endpoint\r\n",
    "- endpoint_name : Nom de l'endpoint associé\r\n",
    "- model : Nom du modèle enregistré à déployer\r\n",
    "- code_configuration : Définit le code source et le script de scoring pour l'inférence\r\n",
    "- environment : Environnement d'exécution contenant les dépendances nécessaires (on utilisera la mêm que l'entraînement)\r\n",
    "- instance_type : Taille de la VM utilisée pour héberger le modèle\r\n",
    "- instance_count : Nombre d'instances pour le scaling horizontal (on en utilisera qu'une)"
   ],
   "metadata": {
    "id": "b1e29665076f"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Temps de déploiement : ~5 min"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = registered_model\r\n",
    "\r\n",
    "from azure.ai.ml.entities import CodeConfiguration\r\n",
    "\r\n",
    "# create an online deployment.\r\n",
    "deployment = ManagedOnlineDeployment(\r\n",
    "    name=\"_____\",\r\n",
    "    endpoint_name=online_endpoint_name,\r\n",
    "    model=model,\r\n",
    "    code_configuration=CodeConfiguration(code=\"./src\", scoring_script=\"score.py\"),\r\n",
    "    environment=curated_env_name,\r\n",
    "    instance_type=\"Standard_DS3_v2\",\r\n",
    "    instance_count=1,\r\n",
    ")\r\n",
    "\r\n",
    "deployment = ml_client.begin_create_or_update(deployment).result()"
   ],
   "outputs": [],
   "metadata": {
    "id": "3e6b04d29c3b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après avoir déployé un modèle sur un endpoint Azure ML, le trafic est initialement à 0% par défaut. Cela signifie qu'aucune requête ne sera routée vers le nouveau déploiement. On peut manuellement mettre le traffic de l'endpoint à 100% sur le portail Azure ou via du code :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Créer une règle de trafic pour le nouveau déploiement\r\n",
    "traffic_rules = {\"_____\": 100}\r\n",
    "\r\n",
    "# Mettre à jour l'endpoint avec les nouvelles règles de trafic\r\n",
    "endpoint.traffic = traffic_rules\r\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q1 : Quel est le coût de la ressource \"Standard_DS3_v2\" à l'heure ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test du modèle\r\n",
    "\r\n",
    "Maintenant que le modèle est entraîné et déployer sur un endpoint on veut vérifier son bon fonctionnement.\r\n",
    "\r\n",
    "Dans le repertoire request, il y a un fichier au format json (sample-request) qui associe la clé \"data\" à une liste comprenant une liste de 784 valeurs représentant une image MNIST en niveau de gris.\r\n",
    "\r\n",
    "On va envoyer ce json à l'endpoint et on devrait recevoir le résultat.\r\n",
    "\r\n",
    "Pour vérifier la bonne réponse du modèle, observons d'abord à quelle valeur est associé l'image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "# 1. Charger le fichier JSON\r\n",
    "with open('request\\sample-request.json') as f:\r\n",
    "    data = json.load(f)\r\n",
    "\r\n",
    "# 2. Extraire les tableaux de pixels\r\n",
    "images_data = data['data']  # Liste de tableaux de 784 valeurs\r\n",
    "\r\n",
    "# 3. Convertir en format image (28x28)\r\n",
    "images = [np.array(img).reshape(28, 28) for img in images_data]\r\n",
    "\r\n",
    "# 4. Affichage avec matplotlib\r\n",
    "plt.figure(figsize=(15, 5))\r\n",
    "for i, (img, img_data) in enumerate(zip(images, images_data)):\r\n",
    "    plt.subplot(1, len(images), i+1)\r\n",
    "    plt.imshow(img, cmap='gray')\r\n",
    "    plt.axis('off')\r\n",
    "\r\n",
    "plt.tight_layout()\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lancement test"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = ml_client.online_endpoints.invoke(\r\n",
    "    endpoint_name=online_endpoint_name,\r\n",
    "    request_file=\"./request/sample-request.json\",\r\n",
    "    deployment_name=\"_____\",\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "id": "1cf1076178fc"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inférence par lot\r\n",
    "\r\n",
    "Nous allons maintenant utiliser le modèle pour faire de la prédiction par lot (batch).\r\n",
    "\r\n",
    "Le déploiement batch traite des volumes de données en asynchrone (idéal pour les gros fichiers), tandis que le déploiement temps réel répond instantanément aux requêtes individuelles (pour les applications interactives)\r\n",
    "\r\n",
    "Le fonctionnement du déploiement est différent pour l'inférence par lot :\r\n",
    "- l'endpoint doit être un `BatchEndpoint`\r\n",
    "- il faut lancer un cluster de calcul via `AmlCompute` à utiliser\r\n",
    "- il faut créer un environnement personnalisé avec l'objet `Environment`\r\n",
    "\r\n",
    "On va alors créer un objet `BatchEndpoint`, qui prend en argument :\r\n",
    "- name : Nom de l'endpoint\r\n",
    "- description : Description de l'endpoint"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_endpoint_name = \"_____\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import BatchEndpoint, BatchDeployment\r\n",
    "\r\n",
    "# Créer un endpoint batch\r\n",
    "batch_endpoint = BatchEndpoint(\r\n",
    "    name=batch_endpoint_name,\r\n",
    "    description=\"Processing en lot des images MNIST\"\r\n",
    ")\r\n",
    "ml_client.begin_create_or_update(batch_endpoint).result()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Création d'un cluster de calcul"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import AmlCompute\r\n",
    "\r\n",
    "cpu_compute = AmlCompute(\r\n",
    "    name=\"_____\",\r\n",
    "    size=\"STANDARD_DS3_V2\",\r\n",
    "    min_instances=0,\r\n",
    "    max_instances=2\r\n",
    ")\r\n",
    "ml_client.compute.begin_create_or_update(cpu_compute)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Créer l'environnement"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from azure.ai.ml.entities import Environment\r\n",
    "\r\n",
    "# Créer un environnement à partir de l'image Docker\r\n",
    "env = Environment(\r\n",
    "    name=\"_____\",\r\n",
    "    image=\"mcr.microsoft.com/azureml/curated/tensorflow-2.16-cuda12:11\",\r\n",
    "    description=\"Environnement TensorFlow pour le traitement batch\"\r\n",
    ")\r\n",
    "    \r\n",
    "# Créer ou mettre à jour l'environnement dans Azure ML\r\n",
    "env = ml_client.environments.create_or_update(env)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Déploiement\r\n",
    "\r\n",
    "On va utiliser l'objet `BatchDeployment` qui prend les mêmes arguments que `ManagedOnlineDeployment`. Cependant, on utilise un script de scoring spécial pour l'inférence par lot.\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "batch_deployment = BatchDeployment(\r\n",
    "    name=\"_____\",\r\n",
    "    endpoint_name=batch_endpoint_name,\r\n",
    "    model=model,\r\n",
    "    code_configuration=CodeConfiguration(code=\"./src\", scoring_script=\"batch_score.py\"),\r\n",
    "    environment=env,\r\n",
    "    compute=cpu_compute.name,  # Cluster dédié\r\n",
    "    instance_count=2\r\n",
    ")\r\n",
    "ml_client.begin_create_or_update(batch_deployment).result()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "La différence entre de l'inférence par lot et par batch est qu'on doit ajouter les données à prédire dans Azure ML avec l'objet `Data`. Les données sont les 5 json du folder batch_request.\r\n",
    "\r\n",
    "Ils contiennent tous une clé \"data\" associé à une liste de 30 listes comprenant 784 valeurs, représentant des images MNIST en niveau de gris.\r\n",
    "Il y 30 images associé à \"data\", soit un total de 150 images à analyser par le modèle."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_data_asset = Data(\r\n",
    "    path=\"./batch_request/\",\r\n",
    "    type=AssetTypes.URI_FOLDER,\r\n",
    "    name=\"_____\",\r\n",
    "    description=\"Données en lot d'images MNIST\",\r\n",
    ")\r\n",
    "ml_client.data.create_or_update(input_data_asset)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Après avoir créer ses données on les utilise comme un `Input`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_asset = ml_client.data.get(name=input_data_asset.name, label=\"latest\")\r\n",
    "input_data_batch = Input(path=data_asset.id)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Lancement du test par lot\r\n",
    "\r\n",
    "Ce test va lancer un job dans Azure qui va classifier toutes nos images. L'éxécution prend en moyenne 12 minutes."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Créer un job batch\r\n",
    "result_batch = ml_client.batch_endpoints.invoke(\r\n",
    "    endpoint_name=batch_endpoint_name,\r\n",
    "    deployment_name=batch_deployment.name,\r\n",
    "    inputs={\"data\":input_data_batch}\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.jobs.get(result_batch.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Récupération des résultats dans un fichiers predictions.csv en les téléchargeant depuis Azure"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.jobs.download(name=result_batch.name, download_path=\".\", output_name=\"score\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluer les résulats\r\n",
    "\r\n",
    "Dans le repertoire batch_request_result, il y a 5 json qui correspondent à la valeur réelle des images MNIST passées en prédiction.\r\n",
    "\r\n",
    "Chaque json associe la clé \"labels\" à une liste comprenant 30 valeurs.\r\n",
    "\r\n",
    "Les scripts suivant vont évaluer nos résultats !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\r\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\r\n",
    "\r\n",
    "def load_json_file(file_path):\r\n",
    "    \"\"\"Load labels from a JSON file\"\"\"\r\n",
    "    with open(file_path, 'r') as f:\r\n",
    "        data = json.load(f)\r\n",
    "    return data[\"labels\"]\r\n",
    "\r\n",
    "def load_predictions(file_path):\r\n",
    "    \"\"\"Load predictions from CSV file\"\"\"\r\n",
    "    predictions = []\r\n",
    "    with open(file_path, 'r') as f:\r\n",
    "        for line in f:\r\n",
    "            list_line=[int(x) for x in line if x.isdigit()]\r\n",
    "            predictions.extend(list_line)\r\n",
    "        print(len(predictions))\r\n",
    "    return predictions\r\n",
    "\r\n",
    "def evaluate(y_true, y_pred):\r\n",
    "    \"\"\"Evaluate predictions against true labels\"\"\"\r\n",
    "    # Calculate accuracy\r\n",
    "    accuracy = accuracy_score(y_true, y_pred)\r\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\r\n",
    "    \r\n",
    "    # Print classification report\r\n",
    "    print(\"Classification Report:\")\r\n",
    "    print(classification_report(y_true, y_pred, digits=4))\r\n",
    "    \r\n",
    "    # Print confusion matrix\r\n",
    "    print(\"Confusion Matrix:\")\r\n",
    "    cm = confusion_matrix(y_true, y_pred)\r\n",
    "    print(cm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "true_labels_paths = [\"batch_request_result\\mnist_labels_batch_1.json\", \"batch_request_result\\mnist_labels_batch_2.json\", \"batch_request_result\\mnist_labels_batch_3.json\",\r\n",
    "\"batch_request_result\\mnist_labels_batch_4.json\", \"batch_request_result\\mnist_labels_batch_5.json\"]\r\n",
    "predictions_path = \"predictions.csv\"\r\n",
    "\r\n",
    "# Load true labels\r\n",
    "true_labels = []\r\n",
    "for path in true_labels_paths:\r\n",
    "    true_labels.extend(load_json_file(path))\r\n",
    "\r\n",
    "# Load predictions\r\n",
    "predictions = load_predictions(predictions_path)\r\n",
    "\r\n",
    "# Evaluate\r\n",
    "print(f\"Evaluating {len(true_labels)} samples...\")\r\n",
    "evaluate(true_labels, predictions)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arrêt du cluster et de l'endpoint par batch"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.compute.begin_delete(cpu_compute.name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.online_endpoints.begin_delete(\r\n",
    "    name=batch_endpoint_name\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Partie 4 : Montée en charge de l'endpoint "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Dans cette partie nous allons tester la capacité de notre endpoint a pouvoir monter en charge. Dans un cas d'usage où nous constuisons une application ayant comme vocation a recevoir beacuoup de connexion, \r\n",
    "nous devons nous assurer que les modèles ont cette capacité a traiter ce traffic. \r\n",
    "\r\n",
    "Ainsi, nous allons utiliser la librairie Locust, elle permet de réaliser des test de charge sur des API. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Ouvrez le script locust.py vérifiez bien que la fonction load_data() fonctionne dans votre cas. Ensuite ajouter les variables nécessaires d'Azure;\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Lancer l'application locust avec la commande $locust -f locustfile.py (!! librairie nécessaire locust, PyYAML, flask !!) "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Renseigner le nombre d'utilisateurs, leurs intervalle d'apparition ainsi que l'URL à surchargé (url de scrore à renseigner)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Réalisez plusieurs test de votre endpoint, vous pouvez cahgner la charge en augmentant le nombre d'utilisateurs faisant des requètes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q2 : A partir de combien d'utilisateurs la réponse médiane met plus de 3 secondes à répondre ? Et à partir de combien d'utilisateurs, on atteint 5 % d'erreur ? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Changeons la ressource sur laquelle le modèle est déployé, il faut aller dans Azure Machine Learning, passer le traffic du déploiement à 0, et enfin supprimer le déploiement. Passons à Standard_DS5_v2, vous pouvez le redéployer à la main directement su rle portail Azure, en choisissant le bon script de scoring, ou en remontant à la cellule du déploiement en temps réel, et en changeant le compute par \"Standard_DS5_v2\"."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q3 : Quelles sont les différences entre les deux VM ?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q4 : Avec cette nouvelle VM : A partir de combien d'utilisateurs la réponse médiane met plus de 3 secondes à répondre ? Et à partir de combien d'utilisateurs, on atteint 5 % d'erreur ? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Q5 : Comment pourrait-on améliorer les résultats de nos tests de charge ? Si vous aviez accès à a n'importe quelle VM, laquelle utiliseriez vous pour deployer votre modèle ? (Le coût est à prendre en compte)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Nettoyage du projet"
   ],
   "metadata": {
    "id": "cleanup:custom"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Arrêt de l'endpoint en temps réel"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ml_client.online_endpoints.begin_delete(\r\n",
    "    name=online_endpoint_name,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk-custom-image-classification-batch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('.venv': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "interpreter": {
   "hash": "6dde1e329f9d10cb9b4738442814353775ca1cf8a57984cd0f84c36155af54c3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}