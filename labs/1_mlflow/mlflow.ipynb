{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# MLOPs on GCP course - MLFlow introduction"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=https://www.headmind.com/wp-content/uploads/2024/01/logo_dark.png width=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<img src=https://www.ensta-paris.fr/profiles/createur_profil/themes/createur/dist/images/logo_ensta_new.jpg.pagespeed.ce.ERsGv8BS3M.jpg width=\"200\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "*Context*\n",
    "\n",
    "Credit risk is the risk that a customer doesn't pay back the money they borrowed from a bank. Banks do credit risk modelling to minimize their expected credit loss. ML models can be trained to classify whether a customer is at risk or not.\n",
    "\n",
    "*Dataset*\n",
    "\n",
    "The German Credit Risk dataset is used.\n",
    "\n",
    "The dataset is anonymized because it contains personal identifiable information (PII) on the bank customers. The features are described in the [dataset documentation](https://archive.ics.uci.edu/dataset/144/statlog+german+credit+data).\n",
    "\n",
    "*Objectives*\n",
    "\n",
    "- Dataset exploration : Using EDA, explore the relevant data \n",
    "- ML implementation : train a Random Forest Classifier with Optuna \n",
    "\n",
    "*Notebook made by Headmind Partners AI & Blockchain*"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Libraries"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### You should have python 3.11.0 installed to run this lab and others correctly !"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%pip install -r requirements.txt"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pickle\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.ensemble import RandomForestClassifier\r\n",
    "from sklearn.inspection import permutation_importance\r\n",
    "from sklearn.utils.class_weight import compute_sample_weight\r\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\r\n",
    "from sklearn.metrics.pairwise import cosine_similarity\r\n",
    "from sklearn.preprocessing import StandardScaler\r\n",
    "from matplotlib import pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn import tree\r\n",
    "import optuna\r\n",
    "import seaborn as sns\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.metrics import roc_auc_score, f1_score\r\n",
    "\r\n",
    "from IPython.display import Image\r\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running MLFlow server\r\n",
    "MLFlow enables us to track several informations on the ML model runs through a UI. To start the server, use the command\r\n",
    "\r\n",
    "```mlflow server --host 127.0.0.1 --port 8080```\r\n",
    "\r\n",
    " from the root of the project\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mlflow\r\n",
    "# By default, the logs will be saved in the current folder. To link your notebook computations to the mlflow server, set the tracking uri to the same uri as the server\r\n",
    "host = \"0.0.0.0\" #TODO\r\n",
    "port = \"6000\" #TODO\r\n",
    "mlflow.set_tracking_uri(uri = f\"http://{host}:{port}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Exploration"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filename = \"data/dataset.parquet\"\r\n",
    "\r\n",
    "df = pd.read_parquet(filename)\r\n",
    "df.head()"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal is to predict if a bank can give a credit to a customer according to its profile\n",
    "\n",
    "Question: Identify the target field"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Identify target field\r\n",
    "#########################\r\n",
    "target_field = \"\" # TODO\r\n",
    "#########################"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Let's rename the target field\r\n",
    "df = df.rename(columns={target_field:'risk'})\r\n",
    "# And change the label values \r\n",
    "df['risk'] = df['risk'].map({1:0,2:1})\r\n",
    "\r\n",
    "y = df['risk']\r\n",
    "X = df.drop(columns=['risk'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a binary classification problem where\n",
    "-  y = 1 if the customer is at risk\n",
    "-  y = 0 if the customer is \"bankable\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In real life banks assess customer risk with more than two values (risky or not risky).\n",
    "\n",
    "In our case, what trick would you suggest to get n risk values (with n>2) ? (with probabilities for instance)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Using seaborn to explore data \n",
    "\n",
    "Correlation matrixes and features distributions according to the credit risk are displayed using the *seaborn* library."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Correlation matrix\r\n",
    "corr = df.corr(numeric_only = True)\r\n",
    "plt.figure(figsize=(12,12))\r\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Question : Do you consider the dataset unbalanced ? Compute the label proportion. If a dataset is unbalanced what are the risks on the model? Which method would you use to manage an unbalanced dataset?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Encoding"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Preliminary data exploration helped us discover all the features in the dataset, their distributions and correlations.\n",
    "\n",
    "The categorical features now have to be encoded"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\r\n",
    "numeric_feat = X.select_dtypes(include=numerics).columns.tolist()\r\n",
    "\r\n",
    "##############################################\r\n",
    "# Pick the right categorical features to encode\r\n",
    "categorical_feat = [\"checking_account_status\", ...] # TODO\r\n",
    "##############################################\r\n",
    "\r\n",
    "onehot_encoder = OneHotEncoder()\r\n",
    "\r\n",
    "# Fit_transform - create a X_enc dataframe from the X dataframe\r\n",
    "X_enc_array = onehot_encoder.fit_transform(X[categorical_feat])\r\n",
    "X_enc = pd.DataFrame(X_enc_array.toarray(), columns=onehot_encoder.get_feature_names_out(input_features=categorical_feat))\r\n",
    "X_enc[numeric_feat] = X[numeric_feat]\r\n",
    "\r\n",
    "display(X_enc.head())\r\n"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is a one-hot encoder? How would it transform the following pandas Series: ['Cat','Cat','Dog','Cat','Bird','Dog']?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(\"data/one_hot_encoder.pkl\", 'wb') as file:\r\n",
    "    pickle.dump(onehot_encoder, file)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ML Modeling"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Train/test split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Question : Split X and y to fit the model. Make sure the risk proportion in the train set are the same as in the test set using the argument *stratify*. Use random_state = 16"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train,X_test,y_train,y_test =  ... # TODO"
   ],
   "outputs": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training an ML model\n",
    "During the rest of this workshop, we'll train a random forest classifier. What other models would be appropriate for the current problem? Justify your answer."
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Training with default hyperparameters\n"
   ],
   "metadata": {},
   "attachments": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Basic configuration\r\n",
    "\r\n",
    "rf_clf = RandomForestClassifier(random_state=42)\r\n",
    "rf_clf.fit(X_train, y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Before running the follwoing script, make sure you have started the server using the command : \r\n",
    "```mlflow server --host 127.0.0.1 --port 8080```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\r\n",
    "with mlflow.start_run(run_name=\"RandomForest_NoOptimization\"):\r\n",
    "    # log params\r\n",
    "    params = rf_clf.get_params()\r\n",
    "    mlflow.log_param(\"n_estimators\", params[\"n_estimators\"])\r\n",
    "    mlflow.log_param(\"bootstrap\", params[\"bootstrap\"])\r\n",
    "    mlflow.log_param(\"min_samples_leaf\", params[\"min_samples_leaf\"])\r\n",
    "    mlflow.log_param(\"max_depth\", params[\"max_depth\"])\r\n",
    "\r\n",
    "    # log metrics\r\n",
    "    y_pred = rf_clf.predict_proba(X_test)[:,1]\r\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\r\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_clf.predict(X_test)))\r\n",
    "    \r\n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\r\n",
    "        registered_model_name=\"sk-learn-random-forest\")\r\n",
    "    \r\n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Using only the MLFlow UI, what are the basic parameters of a random forest classifier? (Justify by writing the path you took in the UI to read them)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimizing hyperparameters by hand\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Based on the results of the optimization, fine-tune the model using the provided code and write each result you obtain in a table"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Modify here to fine-tune the model\r\n",
    "params = {\r\n",
    "    \"n_estimators\":1, # TODO\r\n",
    "    \"bootstrap\":False,\r\n",
    "    \"min_samples_leaf\":1,\r\n",
    "    \"max_depth\":1,\r\n",
    "}\r\n",
    "\r\n",
    "rf_clf = RandomForestClassifier(**params,random_state=42)\r\n",
    "rf_clf.fit(X_train, y_train)\r\n",
    "\r\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\r\n",
    "with mlflow.start_run(run_name=\"RandomForest_manualOptim\"):\r\n",
    "    # log params\r\n",
    "    params = rf_clf.get_params()\r\n",
    "    mlflow.log_param(\"n_estimators\", params[\"n_estimators\"])\r\n",
    "    mlflow.log_param(\"bootstrap\", params[\"bootstrap\"])\r\n",
    "    mlflow.log_param(\"min_samples_leaf\", params[\"min_samples_leaf\"])\r\n",
    "    mlflow.log_param(\"max_depth\", params[\"max_depth\"])\r\n",
    "\r\n",
    "    # log metrics\r\n",
    "    y_pred = rf_clf.predict_proba(X_test)[:,1]\r\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\r\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_clf.predict(X_test)))\r\n",
    "    \r\n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\r\n",
    "        registered_model_name=\"sk-learn-random-forest\")\r\n",
    "    \r\n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Optimizing hyperparameters with Optuna"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a href=https://optuna.readthedocs.io/en/stable/index.html> Optuna </a> is a hyperparameter fine-tuning framework.\n",
    "\n",
    "To use it, you first define a trial, a scoring function, and a set of hyperparameters to fine-tune, using 'suggest' methods.\n",
    "\n",
    "Then, you choose an heuristic and optuna will try different sets of hyperparameters and log the KPIs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify = y_train, random_state=16)\r\n",
    "\r\n",
    "def objective_rf(trial):\r\n",
    "    rf_params = {\r\n",
    "            # Parameter space definition\r\n",
    "            #################################################################\r\n",
    "            # TODO: based on your previous results, set a \r\n",
    "            'n_estimators' : trial.suggest_int('n_estimators',low=...,high=...),\r\n",
    "            'max_depth' : trial.suggest_int('max_depth',low=...,high=...),\r\n",
    "            'bootstrap' : trial.suggest_categorical('bootstrap', []),\r\n",
    "            'min_samples_leaf' : trial.suggest_float(\"min_samples_leaf\", low = ..., high = ...)\r\n",
    "            #################################################################\r\n",
    "            }\r\n",
    "\r\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\r\n",
    "    rf_classifier.set_params(**rf_params)\r\n",
    "\r\n",
    "    rf_classifier.fit(X_train, y_train)\r\n",
    "\r\n",
    "    # Log metrics\r\n",
    "    y_pred = rf_classifier.predict(X_val)\r\n",
    "    score=f1_score(y_val, y_pred)\r\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_val,y_pred))\r\n",
    "    mlflow.log_metric(\"f1-score\", score)\r\n",
    "    return score"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\r\n",
    "full_objective = lambda trial: objective_rf(trial)\r\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\r\n",
    "with mlflow.start_run(run_name=\"RandomForest_Finetuning_exp\"):\r\n",
    "    study.optimize(full_objective, n_trials=30, timeout=600)\r\n",
    "rf_params = study.best_trial.params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "What is the difference between a train, validation, and test set. What are the risks if there is overlapping between the validation and test set?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\r\n",
    "rf_classifier.set_params(**rf_params)\r\n",
    "\r\n",
    "X_train_val, y_train_val = pd.concat((X_train, X_val)), pd.concat((y_train, y_val))\r\n",
    "\r\n",
    "rf_classifier.fit(X_train_val, y_train_val)\r\n",
    "with mlflow.start_run(run_name=\"RandomForest_Optimization\"):\r\n",
    "    # log params\r\n",
    "    mlflow.log_param(\"n_estimators\", rf_params[\"n_estimators\"])\r\n",
    "    mlflow.log_param(\"min_samples_leaf\", rf_params[\"min_samples_leaf\"])\r\n",
    "    mlflow.log_param(\"max_depth\", rf_params[\"max_depth\"])\r\n",
    "    mlflow.log_param('max_features', rf_params['max_features'])\r\n",
    "\r\n",
    "    # log metrics\r\n",
    "    y_pred = rf_classifier.predict_proba(X_test)[:,1]\r\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\r\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_classifier.predict(X_test)))\r\n",
    "    \r\n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\r\n",
    "        registered_model_name=\"sk-learn-random-forest-finetuned\")\r\n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieves a model logged on MLFlow - on run_id"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mlflow\r\n",
    "from IPython.display import display\r\n",
    "\r\n",
    "experiment_name = [\"finetune-creditrisk\"]\r\n",
    "run_name = \"RandomForest_Optimization\"\r\n",
    "\r\n",
    "# Search for the run using the experiment name and run name\r\n",
    "runs = mlflow.search_runs(experiment_names=experiment_name)\r\n",
    "\r\n",
    "display(runs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "last_run_id = runs.loc[runs[\"tags.mlflow.runName\"] == run_name]\r\n",
    "last_run_id.sort_values(by = [\"end_time\"], ascending=False, inplace=True)\r\n",
    "run_id = last_run_id.iloc[0][\"run_id\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Retrieves a model from MLFlow\r\n",
    "model = mlflow.sklearn.load_model(f\"runs:/{run_id}/sklearn-model\")\r\n",
    "model.predict(X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Upgrades the model status"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from mlflow import MlflowClient\r\n",
    "\r\n",
    "client = MlflowClient()\r\n",
    "client.transition_model_version_stage(\r\n",
    "    name=\"sk-learn-random-forest-finetuned\", version=# TODO: choose the latest version based on the UI, stage=\"Production\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Retrieves the model from the status"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import mlflow\r\n",
    "\r\n",
    "model_name = # TODO: Load the right model\r\n",
    "model_version = # TODO: load the right version\r\n",
    "model = mlflow.sklearn.load_model(model_uri=f\"models:/{model_name}/{model_version}\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}